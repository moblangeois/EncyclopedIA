{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "class SimpleKnowledgeGraph:\n",
    "    def __init__(self, openai_api_key):\n",
    "        self.client = OpenAI(api_key=openai_api_key)\n",
    "        self.df = None\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        self.df = pd.read_csv(file_path, sep='\\t')\n",
    "        self.df['content'] = self.df['content'].astype(str).replace('nan', '')\n",
    "        self.df = self.df[self.df['content'].str.strip() != '']\n",
    "        self.df['references'] = self.df['content'].apply(self.extract_references)\n",
    "\n",
    "    def create_embeddings(self):\n",
    "        tqdm.pandas()\n",
    "        self.df['embedding'] = self.df['content'].progress_apply(self.get_embedding)\n",
    "\n",
    "    def extract_references(self, text):\n",
    "        pattern = r'Voyez\\s+([^.,;]+)'\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        return [match.strip() for match in matches]\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        max_tokens = 8000  # Laissons une marge de sécurité\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        \n",
    "        if len(tokens) <= max_tokens:\n",
    "            return self._get_embedding_for_text(text)\n",
    "        else:\n",
    "            # Diviser le texte en morceaux\n",
    "            chunks = []\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "            for token in tokens:\n",
    "                if current_length + 1 > max_tokens:\n",
    "                    chunks.append(self.tokenizer.decode(current_chunk))\n",
    "                    current_chunk = [token]\n",
    "                    current_length = 1\n",
    "                else:\n",
    "                    current_chunk.append(token)\n",
    "                    current_length += 1\n",
    "            if current_chunk:\n",
    "                chunks.append(self.tokenizer.decode(current_chunk))\n",
    "            \n",
    "            # Obtenir l'embedding pour chaque morceau\n",
    "            embeddings = [self._get_embedding_for_text(chunk) for chunk in chunks]\n",
    "            \n",
    "            # Faire la moyenne des embeddings\n",
    "            avg_embedding = np.mean(embeddings, axis=0)\n",
    "            return avg_embedding.tolist()\n",
    "\n",
    "    def _get_embedding_for_text(self, text):\n",
    "        response = self.client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\", input=text\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "\n",
    "    def export_to_jsonld(self, file_path):\n",
    "        jsonld_data = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            # Utilisation de l'ID `id_enccre` pour garantir l'unicité de l'URL\n",
    "            article_url = f\"http://enccre.academie-sciences.fr/encyclopedie/article/{row['id_enccre']}/\"\n",
    "\n",
    "            node_data = {\n",
    "                \"@context\": \"http://schema.org\",\n",
    "                \"@type\": \"Article\",\n",
    "                \"@id\": row['id_enccre'],\n",
    "                \"url\": article_url,\n",
    "                \"title\": row.get('head', ''),\n",
    "                \"authors\": row.get('author', 'Unknown'),\n",
    "                \"content\": row['content'],\n",
    "                \"references\": row['references'],\n",
    "                # Conversion de l'embedding NumPy en liste Python\n",
    "                \"embedding\": row.get('embedding', []).tolist() if isinstance(row.get('embedding', []), np.ndarray) else row.get('embedding', [])\n",
    "            }\n",
    "\n",
    "            # Creating triples for knowledge graph\n",
    "            triples = []\n",
    "            # Relation: is_written_by\n",
    "            triples.append({\n",
    "                \"subject\": node_data[\"@id\"],\n",
    "                \"predicate\": \"is_written_by\",\n",
    "                \"object\": row.get('author', 'Unknown')\n",
    "            })\n",
    "            \n",
    "            # Relation: belongs_to_domain\n",
    "            triples.append({\n",
    "                \"subject\": node_data[\"@id\"],\n",
    "                \"predicate\": \"belongs_to_domain\",\n",
    "                \"object\": row.get('domaine_enccre', 'Unknown')\n",
    "            })\n",
    "            \n",
    "            # Relation: references other articles\n",
    "            for ref in row['references']:\n",
    "                triples.append({\n",
    "                    \"subject\": node_data[\"@id\"],\n",
    "                    \"predicate\": \"references\",\n",
    "                    \"object\": ref\n",
    "                })\n",
    "            \n",
    "            node_data[\"triples\"] = triples\n",
    "            jsonld_data.append(node_data)\n",
    "\n",
    "        # Sauvegarde du fichier JSON-LD\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(jsonld_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"Graphe exporté au format JSON-LD dans {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:29<00:00,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphe exporté au format JSON-LD dans data/EDdA_knowledge_graph.jsonld\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dotenv.load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "kg = SimpleKnowledgeGraph(openai_api_key) # Dimension à spécifier\n",
    "\n",
    "kg.load_data(\"data/EDdA_dataframe_sample.tsv\")\n",
    "kg.create_embeddings()\n",
    "\n",
    "kg.export_to_jsonld(\"data/EDdA_knowledge_graph.jsonld\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
