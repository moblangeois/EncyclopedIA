{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import dotenv\n",
    "import pickle\n",
    "\n",
    "class SimpleKnowledgeGraph:\n",
    "    def __init__(self, openai_api_key, dimensions=512, backup_file='backup.pkl'):\n",
    "        self.client = OpenAI(api_key=openai_api_key)\n",
    "        self.df = None\n",
    "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self.dimensions = dimensions  # dimension à spécifier\n",
    "        self.backup_file = backup_file\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        if os.path.exists(self.backup_file):\n",
    "            print(f\"Chargement de la sauvegarde depuis {self.backup_file}\")\n",
    "            self.load_backup()\n",
    "        else:\n",
    "            print(f\"Chargement des données depuis {file_path}\")\n",
    "            self.df = pd.read_csv(file_path, sep='\\t')\n",
    "            self.df['content'] = self.df['content'].astype(str).replace('nan', '')\n",
    "            self.df = self.df[self.df['content'].str.strip() != '']\n",
    "            self.df['references'] = self.df['content'].apply(self.extract_references)\n",
    "\n",
    "    def create_embeddings(self, save_every=100):\n",
    "        tqdm.pandas()\n",
    "        \n",
    "        # Initialiser la colonne 'embedding' si elle n'existe pas\n",
    "        if 'embedding' not in self.df.columns:\n",
    "            self.df['embedding'] = None  # Initialise avec des valeurs None\n",
    "        \n",
    "        start_index = self.get_start_index_for_embeddings()\n",
    "        \n",
    "        for i in tqdm(range(start_index, len(self.df))):\n",
    "            # Calculer l'embedding pour le texte\n",
    "            embedding = self.get_embedding(self.df.at[i, 'content'])\n",
    "            \n",
    "            # Assigner l'embedding sous forme de liste\n",
    "            self.df.at[i, 'embedding'] = embedding\n",
    "            \n",
    "            # Sauvegarde périodique tous les 'save_every' lignes\n",
    "            if i % save_every == 0 and i > 0:\n",
    "                self.save_backup()\n",
    "        \n",
    "        # Sauvegarde finale à la fin\n",
    "        self.save_backup()\n",
    "\n",
    "    def extract_references(self, text):\n",
    "        pattern = r'Voyez\\s+([^.,;]+)'\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        return [match.strip() for match in matches]\n",
    "\n",
    "    def normalize_l2(self, x):\n",
    "        x = np.array(x)\n",
    "        if x.ndim == 1:\n",
    "            norm = np.linalg.norm(x)\n",
    "            if norm == 0:\n",
    "                return x\n",
    "            return x / norm\n",
    "        else:\n",
    "            norm = np.linalg.norm(x, 2, axis=1, keepdims=True)\n",
    "            return np.where(norm == 0, x, x / norm)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        max_tokens = 8000  # Laissons une marge de sécurité\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        \n",
    "        if len(tokens) <= max_tokens:\n",
    "            return self._get_embedding_for_text(text)\n",
    "        else:\n",
    "            # Diviser le texte en morceaux\n",
    "            chunks = []\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "            for token in tokens:\n",
    "                if current_length + 1 > max_tokens:\n",
    "                    chunks.append(self.tokenizer.decode(current_chunk))\n",
    "                    current_chunk = [token]\n",
    "                    current_length = 1\n",
    "                else:\n",
    "                    current_chunk.append(token)\n",
    "                    current_length += 1\n",
    "            if current_chunk:\n",
    "                chunks.append(self.tokenizer.decode(current_chunk))\n",
    "            \n",
    "            # Obtenir l'embedding pour chaque morceau\n",
    "            embeddings = [self._get_embedding_for_text(chunk) for chunk in chunks]\n",
    "            \n",
    "            # Faire la moyenne des embeddings\n",
    "            avg_embedding = np.mean(embeddings, axis=0)\n",
    "            return avg_embedding.tolist()\n",
    "\n",
    "    def _get_embedding_for_text(self, text):\n",
    "        # Générer un embedding en spécifiant la dimension souhaitée (ex : 512)\n",
    "        response = self.client.embeddings.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-3-small\",  # Utiliser un modèle plus performant\n",
    "            dimensions=self.dimensions  # Réduction à la dimension souhaitée\n",
    "        )\n",
    "        return self.normalize_l2(response.data[0].embedding)  # Normaliser les dimensions réduites\n",
    "\n",
    "    def export_to_jsonld(self, file_path):\n",
    "        jsonld_data = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            # Utilisation de l'ID `id_enccre` pour garantir l'unicité de l'URL\n",
    "            article_url = f\"http://enccre.academie-sciences.fr/encyclopedie/article/{row['id_enccre']}/\"\n",
    "\n",
    "            node_data = {\n",
    "                \"@context\": \"http://schema.org\",\n",
    "                \"@type\": \"Article\",\n",
    "                \"@id\": article_url,  # Utilisation de l'URL avec `id_enccre` pour garantir l'unicité\n",
    "                \"title\": row.get('head', ''),\n",
    "                \"authors\": row.get('author', 'Unknown'),\n",
    "                \"content\": row['content'],\n",
    "                \"references\": row['references'],\n",
    "                # Conversion de l'embedding NumPy en liste Python\n",
    "                \"embedding\": row.get('embedding', []).tolist() if isinstance(row.get('embedding', []), np.ndarray) else row.get('embedding', [])\n",
    "            }\n",
    "\n",
    "            # Creating triples for knowledge graph\n",
    "            triples = []\n",
    "            # Relation: is_written_by\n",
    "            triples.append({\n",
    "                \"subject\": node_data[\"@id\"],\n",
    "                \"predicate\": \"is_written_by\",\n",
    "                \"object\": row.get('author', 'Unknown')\n",
    "            })\n",
    "            \n",
    "            # Relation: belongs_to_domain\n",
    "            triples.append({\n",
    "                \"subject\": node_data[\"@id\"],\n",
    "                \"predicate\": \"belongs_to_domain\",\n",
    "                \"object\": row.get('domaine_enccre', 'Unknown')\n",
    "            })\n",
    "            \n",
    "            # Relation: references other articles\n",
    "            for ref in row['references']:\n",
    "                triples.append({\n",
    "                    \"subject\": node_data[\"@id\"],\n",
    "                    \"predicate\": \"references\",\n",
    "                    \"object\": ref\n",
    "                })\n",
    "            \n",
    "            node_data[\"triples\"] = triples\n",
    "            jsonld_data.append(node_data)\n",
    "\n",
    "        # Sauvegarde du fichier JSON-LD\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(jsonld_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"Graphe exporté au format JSON-LD dans {file_path}\")\n",
    "    \n",
    "    def save_backup(self):\n",
    "        with open(self.backup_file, 'wb') as f:\n",
    "            pickle.dump(self.df, f)\n",
    "        print(f\"Backup sauvegardé dans {self.backup_file}\")\n",
    "\n",
    "    def load_backup(self):\n",
    "        with open(self.backup_file, 'rb') as f:\n",
    "            self.df = pickle.load(f)\n",
    "        print(f\"Backup chargé depuis {self.backup_file}\")\n",
    "    \n",
    "    def get_start_index_for_embeddings(self):\n",
    "        if 'embedding' in self.df.columns:\n",
    "            # Reprendre à partir du premier embedding manquant\n",
    "            return self.df['embedding'].isna().idxmax()\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données depuis data/EDdA_dataframe_cleaned.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/74157 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'embedding'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py:4561\u001b[0m, in \u001b[0;36mDataFrame._set_value\u001b[1;34m(self, index, col, value, takeable)\u001b[0m\n\u001b[0;32m   4560\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4561\u001b[0m     icol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4562\u001b[0m     iindex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(index)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'embedding'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m kg \u001b[38;5;241m=\u001b[39m SimpleKnowledgeGraph(openai_api_key, dimensions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m)  \u001b[38;5;66;03m# Dimension à spécifier\u001b[39;00m\n\u001b[0;32m      6\u001b[0m kg\u001b[38;5;241m.\u001b[39mload_data(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/EDdA_dataframe_cleaned.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mkg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m kg\u001b[38;5;241m.\u001b[39mexport_to_jsonld(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/EDdA_knowledge_graph.jsonld\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 36\u001b[0m, in \u001b[0;36mSimpleKnowledgeGraph.create_embeddings\u001b[1;34m(self, save_every)\u001b[0m\n\u001b[0;32m     33\u001b[0m start_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_start_index_for_embeddings()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(start_index, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf))):\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_embedding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mat[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Sauvegarde périodique tous les 'save_every' lignes\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m save_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexing.py:2586\u001b[0m, in \u001b[0;36m_AtIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mloc[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m   2584\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m-> 2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexing.py:2542\u001b[0m, in \u001b[0;36m_ScalarAccessIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim:\n\u001b[0;32m   2540\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough indexers for scalar access (setting)!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2542\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py:4575\u001b[0m, in \u001b[0;36mDataFrame._set_value\u001b[1;34m(self, index, col, value, takeable)\u001b[0m\n\u001b[0;32m   4573\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[index, col] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m   4574\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4575\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m   4576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_item_cache\u001b[38;5;241m.\u001b[39mpop(col, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   4578\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidIndexError \u001b[38;5;28;01mas\u001b[39;00m ii_err:\n\u001b[0;32m   4579\u001b[0m     \u001b[38;5;66;03m# GH48729: Seems like you are trying to assign a value to a\u001b[39;00m\n\u001b[0;32m   4580\u001b[0m     \u001b[38;5;66;03m# row when only scalar options are permitted\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexing.py:911\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    910\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[1;32m--> 911\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexing.py:1874\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1868\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(arr, np\u001b[38;5;241m.\u001b[39mndarray)\n\u001b[0;32m   1869\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1870\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arr) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1871\u001b[0m ):\n\u001b[0;32m   1872\u001b[0m     \u001b[38;5;66;03m# NumPy 1.25 deprecation: https://github.com/numpy/numpy/pull/10615\u001b[39;00m\n\u001b[0;32m   1873\u001b[0m     arr \u001b[38;5;241m=\u001b[39m arr[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m-> 1874\u001b[0m \u001b[43mempty_value\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m arr\n\u001b[0;32m   1875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj[key] \u001b[38;5;241m=\u001b[39m empty_value\n\u001b[0;32m   1876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "dotenv.load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "kg = SimpleKnowledgeGraph(openai_api_key, dimensions=1024)  # Dimension à spécifier\n",
    "\n",
    "kg.load_data(\"data/EDdA_dataframe_cleaned.tsv\")\n",
    "kg.create_embeddings()\n",
    "\n",
    "kg.export_to_jsonld(\"data/EDdA_knowledge_graph.jsonld\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
