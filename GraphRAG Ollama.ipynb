{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        self.client = chromadb.Client()\n",
    "        self.collection = self.client.create_collection(name=\"encyclopedia\")\n",
    "\n",
    "    def process_tsv(self, file_path, chunk_size=100):\n",
    "        logging.info(\"Début du chargement et traitement du fichier TSV...\")\n",
    "        all_data = []\n",
    "        total_rows = sum(1 for _ in open(file_path, 'r', encoding='utf-8'))\n",
    "        processed_rows = 0\n",
    "\n",
    "        try:\n",
    "            for chunk in pd.read_csv(file_path, sep='\\t', chunksize=chunk_size):\n",
    "                logging.info(f\"Traitement du chunk {processed_rows}-{processed_rows+len(chunk)}\")\n",
    "                chunk['content'] = chunk['content'].astype(str).replace('nan', '')\n",
    "                chunk = chunk[chunk['content'].str.strip() != '']\n",
    "                all_data.append(chunk)\n",
    "\n",
    "                for _, row in chunk.iterrows():\n",
    "                    try:\n",
    "                        embedding = ollama.embeddings(model=\"mxbai-embed-large\", prompt=row['content'])['embedding']\n",
    "                        self.collection.add(\n",
    "                            ids=[str(row['id_enccre'])],\n",
    "                            embeddings=[embedding],\n",
    "                            documents=[row['content']],\n",
    "                            metadatas=[{\n",
    "                                'volume': row['volume'],\n",
    "                                'numero': row['numero'],\n",
    "                                'head': row['head'],\n",
    "                                'author': row['author'],\n",
    "                                'domaine_enccre': row['domaine_enccre']\n",
    "                            }]\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Erreur lors du traitement de l'article {row['id_enccre']}: {e}\")\n",
    "                        continue\n",
    "\n",
    "                processed_rows += len(chunk)\n",
    "                logging.info(f\"Progression: {processed_rows}/{total_rows} lignes traitées\")\n",
    "\n",
    "            df = pd.concat(all_data, ignore_index=True)\n",
    "            logging.info(\"Traitement du fichier TSV terminé\")\n",
    "            return df, self.collection\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Une erreur s'est produite lors du traitement du fichier TSV: {e}\")\n",
    "            raise\n",
    "\n",
    "    def query(self, query: str):\n",
    "        if self.query_engine is None:\n",
    "            raise ValueError(\"Le moteur de requête n'a pas été initialisé. Exécutez d'abord process_tsv().\")\n",
    "        return self.query_engine.query(query)\n",
    "\n",
    "class KnowledgeGraph:\n",
    "    def __init__(self):\n",
    "        self.graph = nx.Graph()\n",
    "        self.edges_threshold = 0.8\n",
    "\n",
    "    def build_graph(self, df):\n",
    "        print(\"Construction du graphe de connaissances...\")\n",
    "        embeddings = []\n",
    "        for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Création des embeddings\"):\n",
    "            node_id = str(row['id_enccre'])\n",
    "            self.graph.add_node(node_id, **row.to_dict())\n",
    "            embedding = ollama.embeddings(model=\"mxbai-embed-large\", prompt=row['content'])['embedding']\n",
    "            embeddings.append(embedding)\n",
    "        \n",
    "        embeddings = np.array(embeddings)\n",
    "        self._add_edges(embeddings)\n",
    "\n",
    "    def _add_edges(self, embeddings):\n",
    "        print(\"Ajout des arêtes au graphe...\")\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "        num_nodes = len(self.graph.nodes)\n",
    "        \n",
    "        for i in tqdm(range(num_nodes), desc=\"Création des arêtes\"):\n",
    "            for j in range(i+1, num_nodes):\n",
    "                similarity_score = similarity_matrix[i][j]\n",
    "                if similarity_score > self.edges_threshold:\n",
    "                    node_i = list(self.graph.nodes)[i]\n",
    "                    node_j = list(self.graph.nodes)[j]\n",
    "                    self.graph.add_edge(node_i, node_j, weight=similarity_score)\n",
    "\n",
    "    def get_related_nodes(self, node_id, depth=1):\n",
    "        related_nodes = set()\n",
    "        current_nodes = {node_id}\n",
    "        \n",
    "        for _ in range(depth):\n",
    "            next_nodes = set()\n",
    "            for node in current_nodes:\n",
    "                next_nodes.update(self.graph.neighbors(node))\n",
    "            related_nodes.update(next_nodes)\n",
    "            current_nodes = next_nodes\n",
    "        \n",
    "        return related_nodes\n",
    "\n",
    "class QueryEngine:\n",
    "    def __init__(self, collection, knowledge_graph):\n",
    "        self.collection = collection\n",
    "        self.knowledge_graph = knowledge_graph\n",
    "\n",
    "    def query(self, query: str):\n",
    "        print(\"Traitement de la requête...\")\n",
    "        embedding = ollama.embeddings(model=\"mxbai-embed-large\", prompt=query)['embedding']\n",
    "        results = self.collection.query(query_embeddings=[embedding], n_results=5)\n",
    "        \n",
    "        context = \"\"\n",
    "        for doc_id, doc_content in zip(results['ids'][0], results['documents'][0]):\n",
    "            related_nodes = self.knowledge_graph.get_related_nodes(doc_id)\n",
    "            for related_id in related_nodes:\n",
    "                related_content = self.knowledge_graph.graph.nodes[related_id]['content']\n",
    "                context += f\"\\n{related_content}\"\n",
    "            context += f\"\\n{doc_content}\"\n",
    "        \n",
    "        response = ollama.generate(\n",
    "            model=\"llama2\",\n",
    "            prompt=f\"Using this context: {context}\\nRespond to this query: {query}\"\n",
    "        )\n",
    "\n",
    "        return response['response'], results['ids'][0], results['documents'][0]\n",
    "\n",
    "class GraphRAG:\n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.knowledge_graph = KnowledgeGraph()\n",
    "        self.query_engine = None\n",
    "\n",
    "    def process_tsv(self, file_path):\n",
    "        if os.path.exists('processed_data.pkl'):\n",
    "            print(\"Chargement des données pré-traitées...\")\n",
    "            with open('processed_data.pkl', 'rb') as f:\n",
    "                df, collection = pickle.load(f)\n",
    "        else:\n",
    "            print(\"Traitement du fichier TSV...\")\n",
    "            df, collection = self.document_processor.process_tsv(file_path)\n",
    "            with open('processed_data.pkl', 'wb') as f:\n",
    "                pickle.dump((df, collection), f)\n",
    "\n",
    "        if os.path.exists('knowledge_graph.pkl'):\n",
    "            print(\"Chargement du graphe de connaissances...\")\n",
    "            with open('knowledge_graph.pkl', 'rb') as f:\n",
    "                self.knowledge_graph = pickle.load(f)\n",
    "        else:\n",
    "            print(\"Construction du graphe de connaissances...\")\n",
    "            self.knowledge_graph.build_graph(df)\n",
    "            with open('knowledge_graph.pkl', 'wb') as f:\n",
    "                pickle.dump(self.knowledge_graph, f)\n",
    "\n",
    "        self.query_engine = QueryEngine(collection, self.knowledge_graph)\n",
    "\n",
    "    def query(self, query: str):\n",
    "        return self.query_engine.query(query)\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self):\n",
    "        self.client = chromadb.Client()\n",
    "        self.collection = self.client.create_collection(name=\"encyclopedia\")\n",
    "\n",
    "    def process_tsv(self, file_path, chunk_size=1000):\n",
    "        print(\"Chargement et traitement du fichier TSV...\")\n",
    "        all_data = []\n",
    "        for chunk in pd.read_csv(file_path, sep='\\t', chunksize=chunk_size):\n",
    "            chunk['content'] = chunk['content'].astype(str).replace('nan', '')\n",
    "            chunk = chunk[chunk['content'].str.strip() != '']\n",
    "            all_data.append(chunk)\n",
    "\n",
    "            for _, row in tqdm(chunk.iterrows(), total=chunk.shape[0], desc=\"Traitement des articles\"):\n",
    "                try:\n",
    "                    embedding = ollama.embeddings(model=\"mxbai-embed-large\", prompt=row['content'])['embedding']\n",
    "                    self.collection.add(\n",
    "                        ids=[str(row['id_enccre'])],\n",
    "                        embeddings=[embedding],\n",
    "                        documents=[row['content']],\n",
    "                        metadatas=[{\n",
    "                            'volume': row['volume'],\n",
    "                            'numero': row['numero'],\n",
    "                            'head': row['head'],\n",
    "                            'author': row['author'],\n",
    "                            'domaine_enccre': row['domaine_enccre']\n",
    "                        }]\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Erreur lors du traitement de l'article {row['id_enccre']}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        df = pd.concat(all_data, ignore_index=True)\n",
    "        return df, self.collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-30 00:30:47,293 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "# Initialisation du système GraphRAG\n",
    "graph_rag = GraphRAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du fichier TSV...\n",
      "Chargement et traitement du fichier TSV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des articles:   0%|          | 0/999 [00:00<?, ?it/s]2024-08-30 00:29:38,748 - INFO - HTTP Request: POST http://192.168.1.49:11434/api/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe noyau s’est bloqué lors de l’exécution du code dans une cellule active ou une cellule précédente. \n",
      "\u001b[1;31mVeuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. \n",
      "\u001b[1;31mCliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. \n",
      "\u001b[1;31mPour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "# Chargement et traitement du fichier TSV de l'Encyclopédie\n",
    "#graph_rag.process_tsv(\"data/EDdA_dataframe_withContent.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement du fichier TSV...\n",
      "Chargement et traitement du fichier TSV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des articles:   0%|          | 0/999 [00:00<?, ?it/s]2024-08-30 00:30:53,436 - INFO - HTTP Request: POST http://192.168.1.49:11434/api/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe noyau s’est bloqué lors de l’exécution du code dans une cellule active ou une cellule précédente. \n",
      "\u001b[1;31mVeuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. \n",
      "\u001b[1;31mCliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. \n",
      "\u001b[1;31mPour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "cProfile.run('graph_rag.process_tsv(\"data/EDdA_dataframe_withContent.tsv\")', 'process_tsv_stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formulation de la requête\n",
    "query = \"Comment Diderot définit le vivant dans l'Encyclopédie ?\"\n",
    "\n",
    "# Exécution de la requête\n",
    "response, related_ids, related_docs = graph_rag.query(query)\n",
    "\n",
    "# Affichage de la réponse\n",
    "print(\"\\nRéponse à la requête :\")\n",
    "print(response)\n",
    "\n",
    "# Affichage des articles connexes\n",
    "print(\"\\nArticles connexes utilisés pour la réponse :\")\n",
    "for id, doc in zip(related_ids, related_docs):\n",
    "    print(f\"ID: {id}\")\n",
    "    print(f\"Contenu: {doc[:200]}...\")  # Affiche les 200 premiers caractères\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
